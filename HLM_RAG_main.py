import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
os.environ["HF_HUB_OFFLINE"] = "1"  # ğŸ‘ˆ å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
# 1 è·å–qwençš„API
load_dotenv()
llm = ChatOpenAI(
    model='qwen-max',
    api_key=os.getenv("Qwen_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    temperature=0.0
)

# 2 æ–‡ä»¶è·¯å¾„
MODEL_PATH = 'models/bge-large-zh-v1.5'
DB_PATH = 'HLM_chroma_db'
RERANKER_PATH = "models/bge-reranker-v2-m3"
TOP_K= 8
# 3 load embeddings model
from langchain_huggingface import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(
    model_name=MODEL_PATH,
    model_kwargs={"device": "cpu"},
    encode_kwargs={'batch_size': 32}
)


# 4 load Chromadb
from langchain_chroma import Chroma
vectorstore = Chroma(persist_directory=DB_PATH,
                     embedding_function=embeddings)


# 5 åŠ è½½é‡æ’åºæ¨¡å‹
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
base_retriever = vectorstore.as_retriever(search_kwargs={"k": TOP_K})
tokenizer = AutoTokenizer.from_pretrained(RERANKER_PATH, local_files_only=True)
model = AutoModelForSequenceClassification.from_pretrained(RERANKER_PATH, local_files_only=True)
model.eval()  # æ¨ç†æ¨¡å¼

def reranker_docs(query: str, docs: list, top_n: int = 2):
    """é‡æ’åºæ–‡æ¡£"""
    if not docs:
        return []

    texts = [doc.page_content for doc in docs]
    pairs = [[query, text] for text in texts]

    with torch.no_grad():
        inputs = tokenizer(
            pairs,
            padding=True,
            truncation=True,
            return_tensors="pt",
            max_length=512
        )
        scores = model(**inputs).logits.view(-1).float().tolist()
    # æŒ‰åˆ†æ•°é™åºæ’åº
    scored_docs = list(zip(docs, scores))
    scored_docs.sort(key=lambda x: x[1], reverse=True)
    return [doc for doc, score in scored_docs[:top_n]]


# 6 æ„é€ prompt
prompt = ChatPromptTemplate.from_messages([
    ("system",
     "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼å—é™çš„ã€Šçº¢æ¥¼æ¢¦ã€‹é—®ç­”å¼•æ“ï¼Œä»…èƒ½åŸºäºç”¨æˆ·æä¾›çš„åŸæ–‡ç‰‡æ®µä½œç­”ã€‚\n"
     "è¯·éµå®ˆä»¥ä¸‹ä¸å¯è¿èƒŒçš„è§„åˆ™ï¼š\n\n"
     "1. **å›ç­”æ¡ä»¶**ï¼šä»…å½“åŒæ—¶æ»¡è¶³ä»¥ä¸‹ä¸¤ç‚¹æ—¶ï¼Œæ‰å…è®¸å›ç­”ï¼š\n"
     "   (a) é—®é¢˜çš„ç­”æ¡ˆ**é€å­—ã€å®Œæ•´åœ°å‡ºç°åœ¨æä¾›çš„æŸä¸€æ®µåŸæ–‡ä¸­**ï¼›\n"
     "   (b) è¯¥åŸæ–‡æ®µè½**ç‹¬ç«‹åŒ…å«å…¨éƒ¨å¿…è¦ä¿¡æ¯**ï¼Œæ— éœ€ç»“åˆå…¶ä»–æ®µè½æˆ–æ¨ç†ã€‚\n\n"
     "2. **å›ç­”æ ¼å¼**ï¼š\n"
     "   - è‹¥æ»¡è¶³æ¡ä»¶ï¼Œç”¨**æœ€ç®€çŸ­çš„ä¸€å¥è¯**ç›´æ¥è¾“å‡ºç­”æ¡ˆï¼Œ**ä¸å¾—æ·»åŠ ä»»ä½•è§£é‡Šã€ä¿®é¥°ã€æ ‡ç‚¹æˆ–å¼•å·**ï¼›\n"
     "   - è‹¥ä¸æ»¡è¶³æ¡ä»¶ï¼Œ**å¿…é¡»ä¸”åªèƒ½è¾“å‡ºä»¥ä¸‹11ä¸ªå­—**ï¼š\n"
     "       æ ¹æ®æä¾›çš„èµ„æ–™æ— æ³•ç¡®å®š\n\n"
     "3. **ç»å¯¹ç¦æ­¢è¡Œä¸º**ï¼ˆè¿åå³é”™è¯¯ï¼‰ï¼š\n"
     "   - ä½¿ç”¨å¸¸è¯†ã€å†å²çŸ¥è¯†ã€äººç‰©å…³ç³»æ¨æ–­ï¼ˆå¦‚â€˜ç‹å¤«äººæ˜¯å®ç‰æ¯äº²â€™ï¼‰ï¼›\n"
     "   - æ€»ç»“ã€æ¦‚æ‹¬ã€æ”¹å†™åŸæ–‡ï¼ˆå¦‚â€˜æ£ºæœ¨æ˜¯å¥½æœ¨æâ€™è€Œéâ€˜æ¨¯æœ¨â€™ï¼‰ï¼›\n"
     "   - å›ç­”éƒ¨åˆ†ä¿¡æ¯ï¼ˆå¦‚åªè¯´â€˜é€šçµå®ç‰â€™è€Œæ¼æ‰â€˜è«å¤±è«å¿˜ï¼Œä»™å¯¿æ’æ˜Œâ€™ï¼‰ï¼›\n"
     "   - è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—ï¼ŒåŒ…æ‹¬â€˜ã€‚â€™ã€â€˜ï¼â€™ã€ç©ºæ ¼ã€æ˜Ÿå·ã€è¯´æ˜ç­‰ã€‚\n\n"
     "4. **ç‰¹åˆ«å¼ºè°ƒ**ï¼š\n"
     "   - å³ä½¿ä½  100% ç¡®ä¿¡ç­”æ¡ˆæ­£ç¡®ï¼Œåªè¦åŸæ–‡æœª**é€å­—å†™å‡º**ï¼Œå°±å¿…é¡»å›ç­”â€˜æ ¹æ®æä¾›çš„èµ„æ–™æ— æ³•ç¡®å®šâ€™ï¼›\n"
     "   - â€˜æ— æ³•ç¡®å®šâ€™ä¸æ˜¯å¤±è´¥ï¼Œè€Œæ˜¯ç³»ç»Ÿè®¾è®¡çš„æ ¸å¿ƒè¦æ±‚ã€‚"
     ),
    ("human", "é—®é¢˜ï¼š{question}\n\nç›¸å…³åŸæ–‡ï¼š\n{context}")
])


# ä¿æŒæ ¼å¼ç»Ÿä¸€
def format_docs(docs):
    # ç§»é™¤å¤šä½™ç©ºæ ¼ï¼Œä¿ç•™åŸå§‹æ–‡æœ¬
    return "\n\n".join([
        f"[å‡ºè‡ªï¼š{d.metadata.get('chapter_title', 'æœªçŸ¥å›ç›®')}]\n{d.page_content.strip()}"
        for d in docs
    ])


# 7 æ„é€ ragé“¾
def rag_with_rerank(question: str) -> str:
    raw_docs = base_retriever.invoke(question)
    ranked_docs = reranker_docs(question, raw_docs)
    context = format_docs(ranked_docs)
    messages = prompt.invoke({"question": question, "context": context})
    response = llm.invoke(messages)
    return StrOutputParser().invoke(response)


# 8 å¯åŠ¨é—®ç­”ç³»ç»Ÿ
print("\nâœ… ã€Šçº¢æ¥¼æ¢¦ã€‹æœ¬åœ°é—®ç­”ç³»ç»Ÿå·²å°±ç»ªï¼")
print("è¾“å…¥é—®é¢˜ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰ï¼š")
# ä¸»å¾ªç¯
while True:
    query = input('<<<:').strip()
    if query.lower() in ['quit', 'exit', 'q']:
        break
    if not query:
        continue
    try:
        answer = rag_with_rerank(query)
        print(f'>>>ï¼š{answer}')
    except Exception as e:
        print(f'>>>å‘ç”Ÿé”™è¯¯ï¼š{e}')
